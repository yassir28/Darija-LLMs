{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate all verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_eng_translations = []\n",
    "for verb in darija_verbs:\n",
    "    completion = client.chat.completions.create(\n",
    "    model =  \"gpt-3.5-turbo\",\n",
    "    messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You will be provided with a verb in Moroccan Darija language,\" +\n",
    "        \" and your task is to translate it into English. Do your best with one output word only, unless it's a phrasal verb.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\":  f'{verb}' \n",
    "    }\n",
    "    ],\n",
    "    temperature=0.3, # deterministic\n",
    "   # max_tokens=1, # in case of a dot\n",
    "    )\n",
    "    gpt_eng_translations.append(completion.choices[0].message.content)\n",
    "\n",
    "print(gpt_eng_translations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metrics for word similarity:\n",
    "\n",
    "- Word Similarity Metrics: \n",
    "Methods such as cosine similarity between word embeddings (vectors representing words) from models like Word2Vec, GloVe, or BERT can be used. These models capture semantic properties of words, so similar words have similar vectors.\n",
    "\n",
    "\n",
    "- WordNet-Based Similarity: \n",
    "WordNet is a lexical database for the English language that groups words into sets of synonyms (synsets) and captures various semantic relations between these sets. Libraries like NLTK in Python provide access to WordNet and functions to compute semantic similarity scores between words based on their synsets' relationships within the WordNet hierarchy. Metrics like path similarity, Wu-Palmer similarity, and Leacock-Chodorow similarity are examples of WordNet-based measures.\n",
    "\n",
    "- Edit Distance: \n",
    "For a more syntactic comparison, edit distance (Levenshtein distance) measures how many single-character edits (insertions, deletions, substitutions) are required to change one word into another. While not a measure of semantic similarity, it's useful for evaluating spelling or typing errors and morphological similarity.\n",
    "\n",
    "\n",
    "###  >> chosen here: wornet synsets, cosine?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metrics for sentence similarity (semantic comparison)\n",
    "\n",
    "- BLEU Score (Bilingual Evaluation Understudy): \n",
    "A standard metric for evaluating a generated text to reference texts. BLEU measures the similarity of the machine translations to one or more reference translations, considering precision of n-grams (sequences of n words).\n",
    "\n",
    "- ROUGE Score (Recall-Oriented Understudy for Gisting Evaluation): \n",
    "Especially useful for evaluating summarization tasks, ROUGE measures overlap between the generated text and the reference texts, including n-gram overlap, word sequence overlap, and word pair overlap.\n",
    "\n",
    "- METEOR Score (Metric for Evaluation of Translation with Explicit ORdering): \n",
    "Attempts to improve upon BLEU by considering synonyms, stemming, and paraphrase matching, as well as sequence alignment for translation evaluation. It balances precision and recall, and it's designed to address some of BLEU's shortcomings.\n",
    "\n",
    "- TER (Translation Edit Rate): \n",
    "Measures the number of edits required to change a system output into one of the references. TER is an error metric where lower scores are better, indicating fewer modifications needed for the translation to match a reference.\n",
    "\n",
    "- BERTScore: \n",
    "Leverages the pre-trained contextual embeddings from BERT (or other transformers) and matches words in candidate and reference sentences by cosine similarity. It's robust against paraphrases since it considers the context of words.\n",
    "\n",
    "- ChrF (Character n-gram F-score): \n",
    "Focuses on character n-grams rather than word n-grams, which can be particularly useful for languages with rich morphology or when dealing with agglutinative languages. It's also useful for languages with less training data available.\n",
    "\n",
    "### >> Metrics chosen: BLEU, METEOR, TER, BERTScore, ChrF (few data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synsets concept\n",
    "why synsets and not synonyms?\n",
    "- Synsets: These are groups of synonyms that share a common meaning. Each synset represents a unique concept and includes a set of synonyms (lemmas) that are interchangeable in some context. Synsets are interconnected by various semantic relations, including hypernyms (more general concepts), hyponyms (more specific concepts), antonyms (opposites), and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('exit.v.01')\n",
      "Synset('go_out.v.02')\n",
      "Synset('go_out.v.03')\n",
      "Synset('go_out.v.04')\n",
      "Synset('go_out.v.05')\n",
      "Synset('go_steady.v.01')\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "synsets1 = wn.synsets('go_out', pos=wn.VERB)\n",
    "for synset in synsets1:\n",
    "    print(synset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- exit, go_out: lemmas of the synset\n",
    "- v: verb, n: noun, a: adjective, s: satellite adj, r: adverb.\n",
    "- 01/02..: a seperate meaning of that lemma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## path similarity\n",
    "How it work in WordNet?\n",
    "\n",
    "The path_similarity function returns a score between 0 and 1, where 1 means the synsets are identical, and a lower score indicates less similarity.\n",
    "\n",
    "The similarity is calculated as follows:\n",
    "\n",
    "similarity = 1 / (path length + 1) â€‹\n",
    " \n",
    "Path length : is the number of edges between the two synsets in the WordNet graph. \n",
    "\n",
    "### examples of path similarity between words to determine threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum path similarity between 'coast' and 'shore' is: 0.5000\n",
      "The maximum path similarity between 'journey' and 'voyage' is: 0.5000\n",
      "The maximum path similarity between 'car' and 'automobile' is: 1.0000\n",
      "The maximum path similarity between 'python' and 'java' is: 0.0714\n",
      "The maximum path similarity between 'forest' and 'tree' is: 0.3333\n",
      "The maximum path similarity between 'love' and 'hate' is: 0.3333\n",
      "The maximum path similarity between 'boy' and 'girl' is: 0.2000\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def max_path_similarity(word1, word2):\n",
    "    \"\"\"Calculate the maximum path similarity between two words.\"\"\"\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "    max_score = 0\n",
    "    for syn1 in synsets1:\n",
    "        for syn2 in synsets2:\n",
    "            score = syn1.path_similarity(syn2)\n",
    "            if score and score > max_score:\n",
    "                max_score = score\n",
    "    return max_score\n",
    "\n",
    "# List of word pairs categorized by their expected similarity\n",
    "word_pairs = [\n",
    "    # Very high similarity - almost synonyms\n",
    "    (\"coast\", \"shore\"),\n",
    "    (\"journey\", \"voyage\"),\n",
    "    (\"car\", \"automobile\"),\n",
    "    \n",
    "    # Moderate to low similarity - part-whole relationship or closely similar concepts\n",
    "    (\"python\", \"java\"),\n",
    "    (\"forest\", \"tree\"),\n",
    "    \n",
    "    # Antonyms or distinct but related concepts\n",
    "    (\"love\", \"hate\"),\n",
    "    (\"boy\", \"girl\"),\n",
    "]\n",
    "\n",
    "# Calculate and print the path similarity for each pair\n",
    "for word1, word2 in word_pairs:\n",
    "    similarity = max_path_similarity(word1, word2)\n",
    "    print(f\"The maximum path similarity between '{word1}' and '{word2}' is: {similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity looks right subjectively =>0.5\n",
    "#### >> instances from the verbs csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go out - exit: Similarity = 0.00, Similar = False\n",
      "understand - comprehend: Similarity = 0.50, Similar = True\n",
      "explain - clarify: Similarity = 0.50, Similar = True\n",
      "learn - study: Similarity = 1.00, Similar = True\n",
      "teach - instruct: Similarity = 1.00, Similar = True\n",
      "hear - listen: Similarity = 1.00, Similar = True\n",
      "see - observe: Similarity = 0.50, Similar = True\n",
      "smell - scent: Similarity = 1.00, Similar = True\n",
      "taste - flavor: Similarity = 0.50, Similar = True\n",
      "touch - feel: Similarity = 0.50, Similar = True\n",
      "go - move: Similarity = 1.00, Similar = True\n",
      "come - arrive: Similarity = 1.00, Similar = True\n",
      "ascend - rise: Similarity = 1.00, Similar = True\n",
      "go down - descend: Similarity = 0.00, Similar = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yassi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import download\n",
    "\n",
    "# Download necessary NLTK data\n",
    "download('wordnet')\n",
    "\n",
    "def word_similarity(word1, word2):\n",
    "    \"\"\"Computing the maximum path similarity score between two words.\"\"\"\n",
    "    synsets1 = wn.synsets(word1,)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "    \n",
    "    max_score = 0\n",
    "    for syn1 in synsets1:\n",
    "        for syn2 in synsets2:\n",
    "            score = syn1.path_similarity(syn2)\n",
    "            if score and score > max_score:\n",
    "                max_score = score\n",
    "    return max_score\n",
    "\n",
    "def evaluate_pairs(word_pairs, threshold=0.5):\n",
    "    \"\"\"Evaluating each pair using WordNet path similarity and a threshold.\"\"\"\n",
    "    results = []\n",
    "    for word1, word2 in word_pairs:\n",
    "        similarity = word_similarity(word1, word2)\n",
    "        results.append((word1, word2, similarity, similarity >= threshold))\n",
    "    return results\n",
    "\n",
    "# Example word pairs\n",
    "word_pairs = [\n",
    "    (\"go out\", \"exit\"),\n",
    "    (\"understand\", \"comprehend\"),\n",
    "    (\"explain\", \"clarify\"),\n",
    "    (\"learn\", \"study\"),\n",
    "    (\"teach\", \"instruct\"),\n",
    "    (\"hear\", \"listen\"),\n",
    "    (\"see\", \"observe\"),\n",
    "    (\"smell\", \"scent\"),\n",
    "    (\"taste\", \"flavor\"),\n",
    "    (\"touch\", \"feel\"),\n",
    "    (\"go\", \"move\"),\n",
    "    (\"come\", \"arrive\"),\n",
    "    (\"ascend\", \"rise\"),\n",
    "    (\"go down\", \"descend\")\n",
    "]\n",
    "\n",
    "# Evaluate the pairs\n",
    "evaluated_pairs = evaluate_pairs(word_pairs)\n",
    "\n",
    "# Print the results\n",
    "for word1, word2, similarity, is_similar in evaluated_pairs:\n",
    "    print(f\"{word1} - {word2}: Similarity = {similarity:.2f}, Similar = {is_similar}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### >> phrasal verbs need further cleaning  from \"go out\" to \"go_out\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps:\n",
    "Can work on a different ipynb if this is not showing.\n",
    "- phrasal verbs in results.cv turned into verbs with tirets 8: \"_\"\n",
    "- cleaning to .. verbs. >> must redo the instruction for openai to more clear not including to + verb.\n",
    "- applying the path similarity function on the synsets.\n",
    "- checks on True percentage of correct match and draw it in a py chart with plotly.\n",
    "- compare another csv containing sentences.\n",
    "- evaluate with cosine similarity, words? if I have time or if I asked professor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "synsets1 = wn.synsets('to_go', pos=wn.VERB)\n",
    "for synset in synsets1:\n",
    "    print(synset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
